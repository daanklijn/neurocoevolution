<html class="h-screen overflow-hidden">
<head>
    <link href="https://unpkg.com/tailwindcss@^2/dist/tailwind.min.css" rel="stylesheet">
</head>
<body>
<div class="w-full p-8 bg-blue-100 h-12vh">
    <img src="vu_logo.png" class="absolute right-10 top-5 h-20">
    <div class="text-2xl font-serif text-center">A Coevolutionary Approach to Deep Multi-agent Reinforcement Learning
    </div>
    <div class="text-xl font-serif text-center">Daan Klijn and Guszti Eiben</div>
</div>
<div class="grid grid-cols-12 grid-rows-12 h-88vh gap-4 p-4 bg-gray-50">
    <!--    <div class="grid grid-cols-3 gap-4 p-4 bg-gray-100">-->
    <div class="bg-white rounded shadow p-4 col-start-1 col-end-5 row-start-1 row-end-7">
        <!--        <div class="row-span-2 col-span-1 bg-white p-8 shadow-lg rounded">-->
        <div class="text-bg mb-2 text-center">Problem statement and contributions</div>
        <div class="text-xs">
            Recent research shows that Deep Neuroevolution is capable of evolving multi-million-parameter DNN's,
            which proved to be particularly useful in the field of Reinforcement Learning.
            This is mainly due to its excellent scalability and simplicity compared to the traditional MDP-based
            Reinforcement Learning
            methods.
            <br>
            <br>
            So far, Deep Neuroevolution has only been applied to complex single-agent problems. As evolutionary
            methods are a natural choice for multi-agent problems, the following question arises: <span
                class="font-bold">can Deep Neuroevolution can also be applied in a complex multi-agent setting?</span>
            <br>
            <br>
            In this work, we describe and validate <span
                class="font-bold">a new approach based on coevolution</span>. To validate our approach, we benchmark
            two Deep coevolutionary Algorithms on a range of multi-agent Atari games and compare our results against
            the results of Ape-X DQN.
            <br>
            <br>
            Our research shows that the combination of Deep Neuroevolution and Coevolution:
            <ul class="list-disc pl-6">
                <li>Can succesfully learn to play multiplayer video games using pixel inputs.</li>
                <li>Outperforms Ape-X DQN in some of the benchmarks.</li>
                <li>Are a viable approach to solving complex multi-agent decision-making problems.</li>
                <li>Might open up a new door in the field of MARL.</li>
            </ul>
        </div>
    </div>
    <div class="bg-white dont-break col-start-1 col-end-5 row-start-7 row-end-13 p-4 rounded shadow">
        <div class="text-bg mb-2 text-center">Our approach</div>
        <div class="text-xs">
            We base our coevolutionary algorithms on the Evolution Strategies from <span class="italic">Salimans et al.</span> [2] and the Genetic Algorithm from <span class="italic">Such et al.</span> [3].
            To prove that even a simple coevolutionary setup, we only transform the original two algorithms to their coevolutionary counterparts, using an evaluator-based approach.


            <!--                In this work, we combine Deep Neuroevolution with a coevolutionary approach. We apply this approach to-->
            <!--                two successful Deep Neuroevolution algorithms; the Evolution Strategies from-->
            <!--                \citet{salimans2017evolution} and the Genetic Algorithm from \citet{such2017deep}. To prove that even a-->
            <!--                simple coevolutionary setup works in combination with DNE, we only transform the original two algorithms-->
            <!--                to their coevolutionary counterparts and limit the number of features we add to them.-->
            <!--                For both algorithms we took an evaluator based approach. For the coevolutionary Evolution Strategies,-->
            <!--                evaluation is perfromed using the last $k$ partents. Similarly, the coevolutionary Genetic Algorithm-->
            <!--                uses the fittest individuals from the last $k$ generations. Below the original algorithms and their-->
            <!--                coevolutionary variants can be found.-->
            <img src="es.png">
            <img src="ga.png">
        </div>
    </div>

    <div class="bg-white dont-break p-4 rounded shadow col-start-5 col-end-10 row-start-1 row-end-8">
        <div class="text-bg text-center mb-2">Experimental setup</div>
        <div class="flex">
            <div class="text-xs w-3/5">
                To validate our approach we used a diverse set of 8 multi-player Atari games from the <span class="italic">PettingZoo
                benchmark</span> [4].
                Below a short overview is given of our experimental setup.
                <ul class="list-disc pl-6">
                    <li><span class="font-bold">Preprocessing</span> - We process the game frames as proposed by <span class="italic">Mnih et al.</span> [1]. Besides that, we also add a form of agent indication
                         as it is essential for each agent to know whether it is player one or player two.
                    </li>
                    <li><span class="font-bold">Network Architecture</span> - We use a DNN that is similar to the large
                        DQN that was used by <span class="italic">Mnih et al.</span> [1] Furthermore, we use virtual batch normalization
                        (VBN) to ensure that the evolved agents are diverse enough. Without
                        VBN, many evolved agents were not diverse and always performed the same actions, regardless of
                        the input.
                    </li>
                    <li><span class="font-bold">Hyperparameters</span> - In comparison to the original ES and GA, we
                        used a slightly higher noise standard deviation and a smaller population size. We were able to
                        get the best results using a population size of 200 and standard deviations of 0.005 (GA) and
                        0.05 (ES). Furthermore, for the number of evaluators <span class="italic">k</span> we found that using <span class="italic">k=3</span> for the GA and <span class="italic">k=1</span> for the ES was enough to
                        ensure accurate evaluations of individuals.
                    </li>
                    <li>
                        <span class="font-bold">Training and evaluation</span> - We trained both coevolutionary algorithms on the eight different games, whereas we start learning from scratch for each of the games. After 200 million frames, we evaluate the trained agents by playing 50 games against a random agent.

                    </li>
                </ul>
            </div>
            <img src="atari.png" class="w-2/5 p-4">
        </div>
    </div>
    <div class="bg-white dont-break p-4 rounded shadow col-start-5 col-end-10 row-start-8 row-end-13">
        <div class="text-bg text-center mb-2">Benchmark results</div>
        <div class="">
            <div class="text-xs">
                <!--                    We trained both coevolutionary algorithms on the eight different games, whereas we start learning from scratch for each of the games. After 200 million frames, we evaluate the trained agents by playing 50 games against a random agent. The training convergence and the average performance against the random agent of both algorithms and Ape-X can be found below.-->
            </div>
            <div class="flex justify-around mt-4">
                <img src="result_table.png" class="h-60 py-2">
                <img src="convergence.png" class="h-64 py-2">
            </div>
        </div>
        <div class="text-xs">

        </div>
    </div>

    <div class="bg-white dont-break p-4 rounded shadow col-start-10 col-end-13 row-start-1 row-end-4">
        <div class="text-bg text-center mb-2">Conclusion</div>
        <div class="text-xs">
            Our results demonstrate that the coevolutionary approach can train agents that can play most of the games to
            some extent while even outperforming Ape-X in several of them. We hypothesize that this approach's
            effectiveness might be due to the EA's being more resistant to the non-stationarity that is introduced by
            the multi-agent aspect of these problems. Although this work only explores the capabilities of simple
            coevolutionary algorithms on a small range of benchmarks, we do believe that this might open up a new door
            in the field of MARL.
        </div>
    </div>
    <div class="bg-white dont-break p-4 rounded shadow col-start-10 col-end-13 row-start-4 row-end-7">
        <div class="text-bg text-center mb-2">Future work</div>
        <div class="text-xs">
            <!--                As our work is limited in terms of algorithm features and number of benchmarks a set of questions remains unanswered. We hope that these questions will be answered by future research.-->
            <ul class="list-disc pl-6">
                <li> How well do these coevolutionary algorithms perform compared to modern Multi-Agent Reinforcement
                    Learning algorithms?
                <li> Can coevolution also be used for complex problems that contain more than two interacting agents?
                </li>
                <li> How will the extension with features like a Pareto Archive or Novelty Search affect the algorithm's
                    performance?
                </li>
            </ul>

        </div>
    </div>
    <div class="bg-white dont-break p-4 rounded shadow col-start-10 col-end-13 row-start-7 row-end-10">
        <div class="text-bg text-center mb-2">References</div>
        <div class="text-xxs">
            [1] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and
            Martin Riedmiller. 2013. Playing atari with deep reinforcement learning. (2013).
            <br>
            [2] Tim Salimans, Jonathan Ho, Xi Chen, Szymon Sidor, and Ilya Sutskever. 2017. Evolution strategies as a
            scalable alternative to reinforcement learning. (2017).
            <br>
            [3] Felipe Petroski Such, Vashisht Madhavan, Edoardo Conti, Joel Lehman, Kenneth O Stanley, and Jeff Clune.
            2017. Deep neuroevolution: Genetic algorithms are a competitive alternative for training deep neural
            networks for reinforcement learning. (2017).
            <br>
            [4] Justin K Terry, Benjamin Black, Ananth Hari, Luis Santos, Clemens Dieffendahl, Niall L Williams, Yashas
            Lokesh, Caroline Horsch, and Praveen Ravi. 2020. PettingZoo: Gym for Multi-Agent Reinforcement Learning.
            (2020).

        </div>
    </div>
    <div class="bg-white dont-break p-4 rounded shadow col-start-10 col-end-13 row-start-10 row-end-13">
        <!--            <div class="text-bg font-bold">Links</div>-->
        <div class="text-xs flex justify-around">
            <div class="w-1/4 text-center">
                GitHub
                <img src="qr_code.png">
            </div>
            <div class="w-1/4 text-center">
                Arxiv Preprint
                <img src="qr_paper.png">
            </div>
            <div class="w-1/4 text-center">
                Gameplay videos
                <img src="qr_videos.png">
            </div>

        </div>
    </div>
</div>
</body>
</html>
<style>
    .grid-rows-12 {
        grid-template-rows: repeat(12,minmax(0,1fr));
    }
    .row-end-8{
        grid-row-end: 8;
    }
    .row-end-9{
        grid-row-end: 9;
    }
    .row-end-10{
        grid-row-end: 10;
    }
    .row-end-11{
        grid-row-end: 11;
    }
    .row-end-12{
        grid-row-end: 12;
    }
    .row-end-13{
        grid-row-end: 13;
    }
    .row-start-8{
        grid-row-start: 8;
    }
    .row-start-9{
        grid-row-start: 9;
    }
    .row-start-10{
        grid-row-start: 10;
    }
    .row-start-11{
        grid-row-start: 11;
    }
    .row-start-12{
        grid-row-start: 12;
    }
    .row-start-13{
        grid-row-start: 13;
    }
    .dont-break {
        break-inside: avoid;
    }
    .h-12vh {
        height: 12vh;
    }
    .h-88vh {
        height: 88vh;
    }
    .text-xxs{
        font-size: 0.5rem;
        line-height: 0.75rem;
    }
</style>
